{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanParsons80/machine_learning_australian_wildlife_strike_cost_estimates/blob/main/code_ml_us_costs__to_aus_costs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrM1wESA66ch"
      },
      "source": [
        "# Preliminary Cells #\n",
        "\n",
        "For importing libraries, data and other preparatory tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuKRhqoI5LFH"
      },
      "outputs": [],
      "source": [
        "### Import cell for required libraries\n",
        "\n",
        "import pandas as pd # for data management\n",
        "import numpy as np\n",
        "import random\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set() # Setting seaborn as default style even if use only matplotlib\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from statsmodels.stats.weightstats import ztest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4Tai5qc52dE"
      },
      "outputs": [],
      "source": [
        "### Import cell for raw data\n",
        "\n",
        "# Australia bird strike data (01/01/2008 - 31/12/2017) downloaded from ATSB webstie (13/04/2022) - https://www.atsb.gov.au/media/5775744/ar-2018-035_birdstrike_data_table.xlsx\n",
        "df_aus_data_raw = pd.read_csv('aus_data_raw.csv')\n",
        "\n",
        "# US bird strike data downloaded from FAA website (29/05/2022) - https://widlife.faa.gov\n",
        "# Unused columns removed and separated into three separate files (to allow upload to Github)\n",
        "df_us_data_part_1 = pd.read_csv('us_data_concat_part_1.csv')\n",
        "df_us_data_part_2 = pd.read_csv('us_data_concat_part_2.csv')\n",
        "df_us_data_part_3 = pd.read_csv('us_data_concat_part_3.csv')\n",
        "\n",
        "df_us_data_raw = pd.concat([df_us_data_part_1, df_us_data_part_2, df_us_data_part_3], ignore_index=True).drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Statistics #\n",
        "\n",
        "This section analyse's the US data to derive summary statistics for Table 1 in the paper."
      ],
      "metadata": {
        "id": "veP3uxGtWPP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cost_data_only = pd.DataFrame()\n",
        "\n",
        "# Need to remove the zero values to avoid -inf in log transform\n",
        "df_us_data_raw.loc[df_us_data_raw['COST_REPAIRS_INFL_ADJ'] == 0, 'COST_REPAIRS_INFL_ADJ'] = np.nan\n",
        "df_us_data_raw.loc[df_us_data_raw['COST_OTHER_INFL_ADJ'] == 0, 'COST_OTHER_INFL_ADJ'] = 0.01\n",
        "\n",
        "# normal\n",
        "df_cost_data_only['cost_repairs_adj'] = df_us_data_raw['COST_REPAIRS_INFL_ADJ']\n",
        "df_cost_data_only['cost_other_adj'] = df_us_data_raw['COST_OTHER_INFL_ADJ']\n",
        "\n",
        "# And now lets remove the nulls\n",
        "all_repair_costs = df_cost_data_only.loc[df_cost_data_only['cost_repairs_adj'].notnull(), 'cost_repairs_adj']\n",
        "all_other_costs = df_cost_data_only.loc[df_cost_data_only['cost_other_adj'].notnull(), 'cost_other_adj']"
      ],
      "metadata": {
        "id": "FbGZTwsrWdm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summaries = pd.DataFrame(columns=['data_group', 'mean', 'std_dev', 'min', 'median', 'max', 'N'])\n",
        "\n",
        "l_data_groups = [all_repair_costs, all_other_costs]\n",
        "l_data_group_names = ['all_repair_costs', 'all_other_costs']\n",
        "\n",
        "for i in range(0,len(l_data_group_names)):\n",
        "  temp_dg_name = l_data_group_names[i]\n",
        "  data_group = l_data_groups[i]\n",
        "  temp_mean = round(data_group.mean())\n",
        "  temp_std_dev = round(data_group.std())\n",
        "  temp_min = round(data_group.min(), 2)\n",
        "  temp_median = round(data_group.median())\n",
        "  temp_max = round(data_group.max())\n",
        "  temp_n = len(data_group)\n",
        "\n",
        "  new_row = {'data_group': temp_dg_name, 'mean': temp_mean, 'std_dev': temp_std_dev, 'min': temp_min, 'median': temp_median, 'max': temp_max, 'N': temp_n}\n",
        "\n",
        "  df_summaries.loc[len(df_summaries)] = new_row\n",
        "\n",
        "df_summaries"
      ],
      "metadata": {
        "id": "42QacJV0aT_k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "d5a96f85-968b-48f6-9b05-fa9d7679c98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         data_group    mean  std_dev   min  median       max     N\n",
              "0  all_repair_costs  171491  1010921  1.00   15304  45432000  4910\n",
              "1   all_other_costs   24839   187126  0.01     716   6925000  4453"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b8f0120-06fa-44c7-bd91-2f53daca6b48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_group</th>\n",
              "      <th>mean</th>\n",
              "      <th>std_dev</th>\n",
              "      <th>min</th>\n",
              "      <th>median</th>\n",
              "      <th>max</th>\n",
              "      <th>N</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>all_repair_costs</td>\n",
              "      <td>171491</td>\n",
              "      <td>1010921</td>\n",
              "      <td>1.00</td>\n",
              "      <td>15304</td>\n",
              "      <td>45432000</td>\n",
              "      <td>4910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>all_other_costs</td>\n",
              "      <td>24839</td>\n",
              "      <td>187126</td>\n",
              "      <td>0.01</td>\n",
              "      <td>716</td>\n",
              "      <td>6925000</td>\n",
              "      <td>4453</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b8f0120-06fa-44c7-bd91-2f53daca6b48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b8f0120-06fa-44c7-bd91-2f53daca6b48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b8f0120-06fa-44c7-bd91-2f53daca6b48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n5r_8Px7GDF"
      },
      "source": [
        "# **Data Pre-processing** #\n",
        "\n",
        "This section brings together each data set and alings them according to common scales. Primarily based on ICAO's IBIS Manual (Doc 9332)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtryq3ExhfLN"
      },
      "outputs": [],
      "source": [
        "### This cell creates two dataframes for use in the ML algorithm\n",
        "\n",
        "df_us_ml_data = pd.DataFrame() # for the US data\n",
        "df_aus_ml_data = pd.DataFrame() # for the AUS data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### To assist with annual and average totals at the end, let's capture the year in which each strike occurred\n",
        "\n",
        "## US Data\n",
        "\n",
        "df_us_ml_data['year'] = df_us_data_raw['INCIDENT_YEAR']\n",
        "\n",
        "## AUS Data\n",
        "\n",
        "df_aus_ml_data['year'] = df_aus_data_raw['year']"
      ],
      "metadata": {
        "id": "uQZoMVmzLg4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBfXEJyq7O6y"
      },
      "outputs": [],
      "source": [
        "### Aircraft Classification pre-processing\n",
        "\n",
        "## This dictionary is for the aircraft class field and is based on field 0011 in the IBIS Manual (Doc 9332, 3rd Ed)\n",
        "ac_dict = {'Aeroplane': 'A',\n",
        "           'Helicopter': 'B',\n",
        "           'Glider': 'C',\n",
        "           'Balloon': 'D',\n",
        "           'Dirigible': 'F',\n",
        "           'Gyroplane': 'I',\n",
        "           'Motor-Glider': 'J',\n",
        "           'Remotely Piloted Aircraft': 'R',\n",
        "           'Other': 'Y',\n",
        "           'Unknown': 'Z'}\n",
        "\n",
        "## US Data\n",
        "df_us_data_raw.loc[df_us_data_raw['AC_CLASS'].isnull(), 'AC_CLASS'] = 'Z' ## The US data contains some null values which should be classed as 'Z - Unknown' as per Doc 9332\n",
        "df_us_ml_data['acft_class'] = df_us_data_raw['AC_CLASS'] # now copy this column into the final ML dataframe\n",
        "\n",
        "## AUS Data\n",
        "df_aus_data_raw.loc[df_aus_data_raw['AircraftType'].isnull(), 'AircraftType'] = 'Unknown' ## The AUS data contains some null values which should be classed as 'Z - Unknown' as per Doc 9332\n",
        "df_aus_ml_data['acft_class'] = df_aus_data_raw['AircraftType'].apply(lambda x: ac_dict.get(x)) # now copy this column into the final ML dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sziDC3W557Pd"
      },
      "outputs": [],
      "source": [
        "### Engine Type pre-processing\n",
        "\n",
        "## This dictionary is for the type of power and is based on field 0014 in the IBIS Manual (Doc 9332, 3rd Ed)\n",
        "eng_dict = {'Piston': 'A', # Reciprocating engine in Doc 9332\n",
        "            'Turboprop': 'B',\n",
        "            'Turbojet': 'C',\n",
        "            'Turbofan': 'D',\n",
        "            'Not Applicable': 'E', # None (glider) in Doc 9332\n",
        "            'Turboshaft': 'F',\n",
        "            'Electric': 'Y', # Other in Doc 9332\n",
        "            'Unknown': 'Z'}\n",
        "\n",
        "## US Data\n",
        "df_us_data_raw.loc[df_us_data_raw['TYPE_ENG'].isnull(), 'TYPE_ENG'] = 'Z' ## The US data contains some null values which should be classed as 'Z - Unknown' as per Doc 9332\n",
        "df_us_ml_data['type_eng'] = df_us_data_raw['TYPE_ENG'] # now copy this column into the final ML dataframe\n",
        "\n",
        "## AUS Data\n",
        "df_aus_data_raw.loc[df_aus_data_raw['EngineType'].isnull(), 'EngineType'] = 'Unknown' ## The AUS data contains some null values which should be classed as 'Z - Unknown' as per Doc 9332\n",
        "df_aus_ml_data['type_eng'] = df_aus_data_raw['EngineType'].apply(lambda x: eng_dict.get(x)) # now copy this column into the final ML dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWEEJLqW9ns0"
      },
      "outputs": [],
      "source": [
        "### Aircraft Mass pre-processing\n",
        "\n",
        "## This dictionary is for the aircraft mass category and is based on field 0012 in the IBIS Manual (Doc 9332, 3rd Ed)\n",
        "mass_dict = {'0-2250 Kg (0-4960 Lbs)': 1.0, # Reciprocating engine in Doc 9332\n",
        "             '2251-5700 Kg (4960-12565 Lbs)': 2.0,\n",
        "             '5701-27000 Kg (12565-59525 Lbs)': 3.0,\n",
        "             '27001-272000 Kg (59525-599650 Lbs)': 4.0,\n",
        "             'Over 272001 Kg (>599650 Lbs)': 5.0,\n",
        "             'Unknown': 'Z'}\n",
        "\n",
        "## US Data\n",
        "df_us_data_raw.loc[df_us_data_raw['AC_MASS'].isnull(), 'AC_MASS'] = 'Z' ## The US data contains some null values which should be classed as 'Z - Unknown' as per Doc 9332\n",
        "df_us_ml_data['mass_cat'] = df_us_data_raw['AC_MASS'] # now copy this column into the final ML dataframe\n",
        "\n",
        "\n",
        "## AUS Data\n",
        "df_aus_data_raw.loc[df_aus_data_raw['MaximumWeightCategory'].isnull(), 'MaximumWeightCategory'] = 'Unknown' ## The AUS data contains some null values which should be classed as 'Z - Unknown' as per Doc 9332\n",
        "df_aus_ml_data['mass_cat'] = df_aus_data_raw['MaximumWeightCategory'].apply(lambda x: mass_dict.get(x)) # now copy this column into the final ML dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWZ-Z3-lpkZQ"
      },
      "outputs": [],
      "source": [
        "### Phase of Flight pre-processing\n",
        "\n",
        "## This dictionary is for the phase of flight and is based on field 0117 in the IBIS Manual (Doc 9332, 3rd Ed) but a few tweaks were required because some non-standard and additional labels were used\n",
        "phase_dict = {'Standing': 'A', # for non-standard label in US data\n",
        "              'Parked': 'A',\n",
        "              'Taxiing': 'B',\n",
        "              'Taxi': 'B', # for non-standard label in US data\n",
        "              'Take-off Run': 'C',\n",
        "              'Takeoff': 'C', # for non-standard label in AUS data\n",
        "              'Climb': 'D',\n",
        "              'Inital Climb': 'D', # for additional non-standard label in AUS data\n",
        "              'En Route': 'E',\n",
        "              'Cruise': 'E', # for non-standard label in AUS data\n",
        "              'Descent': 'F',\n",
        "              'Approach': 'G',\n",
        "              'Arrival': 'G', # for additional non-standard label in AUS data\n",
        "              'Landing Roll': 'H',\n",
        "              'Landing': 'H', # for non-standard label in AUS data\n",
        "              'Maneuvering/airwork': 'I', # for unassigned label in AUS data\n",
        "              'Local': 'I', # for unassigned but similar label in US data\n",
        "              'None': 'Z', # for unassigned label in AUS data\n",
        "              'Unknown': 'Z' } # although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "\n",
        "## US Data\n",
        "df_us_data_raw.loc[df_us_data_raw['PHASE_OF_FLIGHT'].isnull(), 'PHASE_OF_FLIGHT'] = 'Unknown' ## The US data contains some null values which should be classed as 'Z - Unknown'\n",
        "df_us_ml_data['phase'] = df_us_data_raw['PHASE_OF_FLIGHT'].apply(lambda x: phase_dict.get(x)) # now copy this column into the final ML dataframe\n",
        "\n",
        "## AUS Data\n",
        "df_aus_data_raw.loc[df_aus_data_raw['PhaseOfFlight'].isnull(), 'PhaseOfFlight'] = 'Unknown' ## The AUS data contains some null values which should be classed as 'Z - Unknown' as per Doc 9332\n",
        "df_aus_ml_data['phase'] = df_aus_data_raw['PhaseOfFlight'].apply(lambda x: phase_dict.get(x)) # now copy this column into the final ML dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K8ixGCuRdC9"
      },
      "outputs": [],
      "source": [
        "### Birds seen pre-processing\n",
        "\n",
        "## This dictionary is for the birds seen and struck fields and is based on fields 0142 & 0143 in the IBIS Manual (Doc 9332, 3rd Ed)\n",
        "num_dict = {'1': 'A',\n",
        "             ' 1' : 'A', # unknown nbsp at start of label in US data\n",
        "             ' 2-10': 'B', # unknown nbsp at start of label in US data\n",
        "             '2 - 10 ': 'B', # structure of the label in AUS data\n",
        "             ' 2 - 10 ': 'B', # a weird alternative in the AUS data\n",
        "             ' 11-100': 'C', # unknown nbsp at start of label in US data\n",
        "             '>10': 'C', # structure of the label in AUS data\n",
        "             ' More than 100': 'D', # unknown nbsp at start of label in US data\n",
        "             ' ': 'Z', # unknown nbsp at start of label in US data and although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "             'None': 'Z', # alternative label in AUS data that means the same as no value\n",
        "             'Unknown': 'Z'} # although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "\n",
        "## US Data\n",
        "df_us_ml_data['birds_seen'] = df_us_data_raw['NUM_SEEN'].apply(lambda x: num_dict.get(x)) # now copy this column into the final ML dataframe\n",
        "\n",
        "## AUS Data\n",
        "df_aus_data_raw.loc[df_aus_data_raw['No of birds seen'].isnull(), 'No of birds seen'] = 'Unknown' ## The AUS data contains some null values which should be classed as 'Z - Unknown' as per Doc 9332\n",
        "df_aus_ml_data['birds_seen'] = df_aus_data_raw['No of birds seen'].apply(lambda x: num_dict.get(x)) # now copy this column into the final ML dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAsZ9RpB0ZAv"
      },
      "outputs": [],
      "source": [
        "### Birds struck pre-processing\n",
        "\n",
        "## This dictionary is for the birds seen and struck fields and is based on fields 0142 & 0143 in the IBIS Manual (Doc 9332, 3rd Ed)\n",
        "num_dict = {'1': 'A',\n",
        "             ' 1' : 'A', # unknown nbsp at start of label in US data\n",
        "             ' 2-10': 'B', # unknown nbsp at start of label in US data\n",
        "             '2 - 10 ': 'B', # structure of the label in AUS data\n",
        "             ' 2 - 10 ': 'B', # a weird alternative in the AUS data\n",
        "             ' 11-100': 'C', # unknown nbsp at start of label in US data\n",
        "             '>10': 'C', # structure of the label in AUS data\n",
        "             ' More than 100': 'D', # unknown nbsp at start of label in US data\n",
        "             ' ': 'Z', # unknown nbsp at start of label in US data and although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "             'None': 'Z', # alternative label in AUS data that means the same as no value\n",
        "             'Unknown': 'Z'} # although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "\n",
        "## US Data\n",
        "df_us_ml_data['birds_struck'] = df_us_data_raw['NUM_STRUCK'].apply(lambda x: num_dict.get(x)) # now copy this column into the final ML dataframe\n",
        "\n",
        "## AUS Data\n",
        "df_aus_ml_data['birds_struck'] = df_aus_data_raw['No of birds struck'].apply(lambda x: num_dict.get(x)) # now copy this column into the final ML dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nWwQhRP452O"
      },
      "outputs": [],
      "source": [
        "### Bird size pre-processing\n",
        "\n",
        "## This dictionary is for the size of bird and is based on field 0144 in the IBIS Manual (Doc 9332, 3rd Ed) * Note: this is a perceived field not a scientific value\n",
        "size_dict = {'small': 'S', # AUS label\n",
        "            'Small': 'S', # US label\n",
        "            'medium': 'M', # AUS label\n",
        "            'Medium': 'M', # US label\n",
        "            'large': 'L', # AUS label\n",
        "            'Large': 'L', # US label\n",
        "            'very large': 'L', # AUS-only label that sits outside Doc 9332\n",
        "            'other': 'Z', # alternative label in AUS data that appears to carry no meaning\n",
        "            'unknown': 'Z'} # although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "\n",
        "## US Data\n",
        "df_us_data_raw.loc[df_us_data_raw['SIZE'].isnull(), 'SIZE'] = 'unknown' ## The US data contains some null values which should be classed as 'Z - unknown'\n",
        "df_us_ml_data['bird_size'] = df_us_data_raw['SIZE'].apply(lambda x: size_dict.get(x)) # now copy this column into the final ML dataframe\n",
        "\n",
        "## AUS Data\n",
        "df_aus_ml_data['bird_size'] = df_aus_data_raw['BirdSize'].apply(lambda x: size_dict.get(x)) # now copy this column into the final ML dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGkAodv8NFdE"
      },
      "outputs": [],
      "source": [
        "### Aircraft damage pre-prepocessing\n",
        "\n",
        "## This dictionary is based on field 0201 in the IBIS Manual (Doc 9332, 3rd Ed) * Note: additional field for uncertain/unknown damage retained from US data\n",
        "damage_dict = {'Nil ': 'N',\n",
        "            'Minor': 'M',\n",
        "            'Unknown': 'M?', # US label retained with AUS unknown considered equivalent as nil appears to be null\n",
        "            'Substantial': 'S',\n",
        "            'Destroyed': 'D',\n",
        "            'Missing': 'Z'} # although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "\n",
        "## US Data\n",
        "df_us_data_raw.loc[df_us_data_raw['DAMAGE_LEVEL'].isnull(), 'DAMAGE_LEVEL'] = 'Z' ## The US data contains some null values which should be classed as 'Z - Missing'\n",
        "df_us_data_raw.loc[((df_us_data_raw['DAMAGE_LEVEL'] == 'N') & (df_us_data_raw['COST_REPAIRS_INFL_ADJ'].notnull())), 'DAMAGE_LEVEL'] = 'M?' # There are some repair costs records with damage level set at nil this needs to be fixed\n",
        "df_us_data_raw.loc[((df_us_data_raw['DAMAGE_LEVEL'] == 'Z') & (df_us_data_raw['COST_REPAIRS_INFL_ADJ'].notnull())), 'DAMAGE_LEVEL'] = 'M?' # There are some repair costs records with damage level set at missing this needs to be fixed\n",
        "df_us_ml_data['damage_level'] = df_us_data_raw['DAMAGE_LEVEL'] # now copy this column into the final ML dataframe\n",
        "\n",
        "## AUS Data\n",
        "df_aus_data_raw.loc[df_aus_data_raw['AircraftDamageLevel'].isnull(), 'AircraftDamageLevel'] = 'Missing' ## The AUS data contains some null values which should be classed as 'Z - Missing'\n",
        "df_aus_ml_data['damage_level'] = df_aus_data_raw['AircraftDamageLevel'].apply(lambda x: damage_dict.get(x)) # now copy this column into the final ML dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1tr0X7BgzF7"
      },
      "outputs": [],
      "source": [
        "### Ingestion pre-processing\n",
        "\n",
        "## This dictionary is based on the binary nature of much of the US data. Doc 9332 expects the numbers of birds ingested in each engine to be recorded but neither the AUS or US data does this.\n",
        "ing_dict = {'No': 0,\n",
        "            'Unknown': 0,\n",
        "            'Yes': 1,\n",
        "            '1 Engine': 1,\n",
        "            '2 Engines': 1}\n",
        "\n",
        "## US Data (doesn't need the dictionary above)\n",
        "df_us_data_raw.loc[df_us_data_raw['INGESTED_OTHER'] == True, 'TEMP_ING'] = 1 # This field was used to capture all ingested data prior to 29/3/2021.\n",
        "df_us_data_raw.loc[df_us_data_raw['ING_ENG1'] == True, 'TEMP_ING'] = 1 # This and the following fields were used to capture whether ingestions were recorded in each engine from 29/3/2021\n",
        "df_us_data_raw.loc[df_us_data_raw['ING_ENG2'] == True, 'TEMP_ING'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['ING_ENG3'] == True, 'TEMP_ING'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['ING_ENG4'] == True, 'TEMP_ING'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_ING'].isnull(), 'TEMP_ING'] = 0\n",
        "df_us_ml_data['ingestion'] = df_us_data_raw['TEMP_ING'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['ingestion'] = df_us_ml_data['ingestion'].astype(int)\n",
        "\n",
        "## AUS Data\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Engine Ingestion'].isnull(), 'Engine Ingestion'] = 'Unknown'\n",
        "df_aus_ml_data['ingestion'] = df_aus_data_raw['Engine Ingestion'].apply(lambda x: ing_dict.get(x)) # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['ingestion'] = df_aus_ml_data['ingestion'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4x8jRD9m53Z"
      },
      "outputs": [],
      "source": [
        "### Component damage pre-processing\n",
        "\n",
        "## This one needs signficiant work up front as it begins as one-hot coded, dummy variables in the US data (albeit as true/false)\n",
        "\n",
        "## US Data\n",
        "# Radome & Nose Separate - for use in testing this ML approach against the established method\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_RAD'] == True, 'TEMP_RAD'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_NOSE'] == True, 'TEMP_NOSE'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_RAD'].isnull(), 'TEMP_RAD'] = 0\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_NOSE'].isnull(), 'TEMP_NOSE'] = 0\n",
        "df_us_ml_data['comp_dam_rad'] = df_us_data_raw['TEMP_RAD'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_rad'] = df_us_ml_data['comp_dam_rad'].astype(int)\n",
        "df_us_ml_data['comp_dam_nose'] = df_us_data_raw['TEMP_NOSE'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_nose'] = df_us_ml_data['comp_dam_nose'].astype(int)\n",
        "\n",
        "# Radome & Nose Combined - for use in comparing to AUS data which does not have Radome as a separate option\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_RAD'] == True, 'TEMP_RAD_NOSE'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_NOSE'] == True, 'TEMP_RAD_NOSE'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_RAD_NOSE'].isnull(), 'TEMP_RAD_NOSE'] = 0\n",
        "df_us_ml_data['comp_dam_rad_nose'] = df_us_data_raw['TEMP_RAD_NOSE'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_rad_nose'] = df_us_ml_data['comp_dam_rad_nose'].astype(int)\n",
        "\n",
        "# Windshield\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_WINDSHLD'] == True, 'TEMP_WSD'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_WSD'].isnull(), 'TEMP_WSD'] = 0\n",
        "df_us_ml_data['comp_dam_wind'] = df_us_data_raw['TEMP_WSD'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_wind'] = df_us_ml_data['comp_dam_wind'].astype(int)\n",
        "\n",
        "# Engines separate\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_ENG1'] == True, 'TEMP_ENG1'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_ENG1'].isnull(), 'TEMP_ENG1'] = 0\n",
        "df_us_ml_data['comp_dam_eng_1'] = df_us_data_raw['TEMP_ENG1'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_eng_1'] = df_us_ml_data['comp_dam_eng_1'].astype(int)\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_ENG2'] == True, 'TEMP_ENG2'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_ENG2'].isnull(), 'TEMP_ENG2'] = 0\n",
        "df_us_ml_data['comp_dam_eng_2'] = df_us_data_raw['TEMP_ENG2'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_eng_2'] = df_us_ml_data['comp_dam_eng_2'].astype(int)\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_ENG3'] == True, 'TEMP_ENG3'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_ENG3'].isnull(), 'TEMP_ENG3'] = 0\n",
        "df_us_ml_data['comp_dam_eng_3'] = df_us_data_raw['TEMP_ENG3'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_eng_3'] = df_us_ml_data['comp_dam_eng_3'].astype(int)\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_ENG4'] == True, 'TEMP_ENG4'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_ENG4'].isnull(), 'TEMP_ENG4'] = 0\n",
        "df_us_ml_data['comp_dam_eng_4'] = df_us_data_raw['TEMP_ENG4'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_eng_4'] = df_us_ml_data['comp_dam_eng_4'].astype(int)\n",
        "\n",
        "# Engines combined - US data identifies each engine but AUS data combines them\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_ENG1'] == True, 'TEMP_ENG_ALL'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_ENG2'] == True, 'TEMP_ENG_ALL'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_ENG3'] == True, 'TEMP_ENG_ALL'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_ENG4'] == True, 'TEMP_ENG_ALL'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_ENG_ALL'].isnull(), 'TEMP_ENG_ALL'] = 0\n",
        "df_us_ml_data['comp_dam_eng_all'] = df_us_data_raw['TEMP_ENG_ALL'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_eng_all'] = df_us_ml_data['comp_dam_eng_all'].astype(int)\n",
        "\n",
        "# Propellers\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_PROP'] == True, 'TEMP_PROP'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_PROP'].isnull(), 'TEMP_PROP'] = 0\n",
        "df_us_ml_data['comp_dam_prop'] = df_us_data_raw['TEMP_PROP'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_prop'] = df_us_ml_data['comp_dam_prop'].astype(int)\n",
        "\n",
        "# Wings & Rotors\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_WING_ROT'] == True, 'TEMP_WING'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_WING'].isnull(), 'TEMP_WING'] = 0\n",
        "df_us_ml_data['comp_dam_wing_rotor'] = df_us_data_raw['TEMP_WING'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_wing_rotor'] = df_us_ml_data['comp_dam_wing_rotor'].astype(int)\n",
        "\n",
        "# Fuselage\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_FUSE'] == True, 'TEMP_FUSE'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_FUSE'].isnull(), 'TEMP_FUSE'] = 0\n",
        "df_us_ml_data['comp_dam_fuselage'] = df_us_data_raw['TEMP_FUSE'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_fuselage'] = df_us_ml_data['comp_dam_fuselage'].astype(int)\n",
        "\n",
        "# Tail\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_TAIL'] == True, 'TEMP_TAIL'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_TAIL'].isnull(), 'TEMP_TAIL'] = 0\n",
        "df_us_ml_data['comp_dam_tail'] = df_us_data_raw['TEMP_TAIL'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_tail'] = df_us_ml_data['comp_dam_tail'].astype(int)\n",
        "\n",
        "# Landing Gear\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_LG'] == True, 'TEMP_LG'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_LG'].isnull(), 'TEMP_LG'] = 0\n",
        "df_us_ml_data['comp_dam_gear'] = df_us_data_raw['TEMP_LG'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_gear'] = df_us_ml_data['comp_dam_gear'].astype(int)\n",
        "\n",
        "# Lights\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_LGHTS'] == True, 'TEMP_LGHTS'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_LGHTS'].isnull(), 'TEMP_LGHTS'] = 0\n",
        "df_us_ml_data['comp_dam_lights'] = df_us_data_raw['TEMP_LGHTS'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_lights'] = df_us_ml_data['comp_dam_lights'].astype(int)\n",
        "\n",
        "# Other\n",
        "df_us_data_raw.loc[df_us_data_raw['DAM_OTHER'] == True, 'TEMP_OTHER'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_OTHER'].isnull(), 'TEMP_OTHER'] = 0\n",
        "df_us_ml_data['comp_dam_other'] = df_us_data_raw['TEMP_OTHER'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_dam_other'] = df_us_ml_data['comp_dam_other'].astype(int)\n",
        "\n",
        "## AUS Data\n",
        "# Nose - assuming that is includes radome\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Nose', 'temp_nose'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_nose'].isnull(), 'temp_nose'] = 0\n",
        "df_aus_ml_data['comp_dam_rad_nose'] = df_aus_data_raw['temp_nose'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_rad_nose'] = df_aus_ml_data['comp_dam_rad_nose'].astype(int)\n",
        "\n",
        "# Windshield\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Windscreen', 'temp_screen'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'windscreen', 'temp_screen'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_screen'].isnull(), 'temp_screen'] = 0\n",
        "df_aus_ml_data['comp_dam_wind'] = df_aus_data_raw['temp_screen'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_wind'] = df_aus_ml_data['comp_dam_wind'].astype(int)\n",
        "\n",
        "# Engines - US data identifies each engine but AUS data combines them\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Engine', 'temp_eng'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_eng'].isnull(), 'temp_eng'] = 0\n",
        "df_aus_ml_data['comp_dam_eng_all'] = df_aus_data_raw['temp_eng'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_eng_all'] = df_aus_ml_data['comp_dam_eng_all'].astype(int)\n",
        "\n",
        "# Propellers\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Propeller', 'temp_prop'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_prop'].isnull(), 'temp_prop'] = 0\n",
        "df_aus_ml_data['comp_dam_prop'] = df_aus_data_raw['temp_prop'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_prop'] = df_aus_ml_data['comp_dam_prop'].astype(int)\n",
        "\n",
        "# Wings & Rotors\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Wing/Rotor', 'temp_wing'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Wing/rotor', 'temp_wing'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_wing'].isnull(), 'temp_wing'] = 0\n",
        "df_aus_ml_data['comp_dam_wing_rotor'] = df_aus_data_raw['temp_wing'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_wing_rotor'] = df_aus_ml_data['comp_dam_wing_rotor'].astype(int)\n",
        "\n",
        "# Fuselage\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Fuselage', 'temp_fuselage'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_fuselage'].isnull(), 'temp_fuselage'] = 0\n",
        "df_aus_ml_data['comp_dam_fuselage'] = df_aus_data_raw['temp_fuselage'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_fuselage'] = df_aus_ml_data['comp_dam_fuselage'].astype(int)\n",
        "\n",
        "# Tail\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Tail', 'temp_tail'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_tail'].isnull(), 'temp_tail'] = 0\n",
        "df_aus_ml_data['comp_dam_tail'] = df_aus_data_raw['temp_tail'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_tail'] = df_aus_ml_data['comp_dam_tail'].astype(int)\n",
        "\n",
        "# Landing Gear\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Landing gear', 'temp_gear'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_gear'].isnull(), 'temp_gear'] = 0\n",
        "df_aus_ml_data['comp_dam_gear'] = df_aus_data_raw['temp_gear'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_gear'] = df_aus_ml_data['comp_dam_gear'].astype(int)\n",
        "\n",
        "# Lights\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Lights', 'temp_lights'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_lights'].isnull(), 'temp_lights'] = 0\n",
        "df_aus_ml_data['comp_dam_lights'] = df_aus_data_raw['temp_lights'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_lights'] = df_aus_ml_data['comp_dam_lights'].astype(int)\n",
        "\n",
        "# Other\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Other', 'temp_other'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'Unknown', 'temp_other'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['Part damaged'] == 'unknown', 'temp_other'] = 1\n",
        "df_aus_data_raw.loc[df_aus_data_raw['temp_other'].isnull(), 'temp_other'] = 0\n",
        "df_aus_ml_data['comp_dam_other'] = df_aus_data_raw['temp_other'] # now copy this column into the final ML dataframe\n",
        "df_aus_ml_data['comp_dam_other'] = df_aus_ml_data['comp_dam_other'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0jvSyghrBrZ"
      },
      "outputs": [],
      "source": [
        "### Component struck pre-processing\n",
        "\n",
        "## This is just for the US data in the testing of the ML technique without this data\n",
        "\n",
        "## US Data\n",
        "# Radome\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_RAD'] == True, 'TEMP_STR_RAD'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_RAD'].isnull(), 'TEMP_STR_RAD'] = 0\n",
        "df_us_ml_data['comp_str_rad'] = df_us_data_raw['TEMP_STR_RAD'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_rad'] = df_us_ml_data['comp_str_rad'].astype(int)\n",
        "\n",
        "# Nose\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_NOSE'] == True, 'TEMP_STR_NOSE'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_NOSE'].isnull(), 'TEMP_STR_NOSE'] = 0\n",
        "df_us_ml_data['comp_str_nose'] = df_us_data_raw['TEMP_STR_NOSE'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_nose'] = df_us_ml_data['comp_str_nose'].astype(int)\n",
        "\n",
        "# Windshield\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_WINDSHLD'] == True, 'TEMP_STR_WSD'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_WSD'].isnull(), 'TEMP_STR_WSD'] = 0\n",
        "df_us_ml_data['comp_str_wind'] = df_us_data_raw['TEMP_STR_WSD'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_wind'] = df_us_ml_data['comp_str_wind'].astype(int)\n",
        "\n",
        "# Engines separate\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_ENG1'] == True, 'TEMP_STR_ENG1'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_ENG1'].isnull(), 'TEMP_STR_ENG1'] = 0\n",
        "df_us_ml_data['comp_str_eng_1'] = df_us_data_raw['TEMP_STR_ENG1'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_eng_1'] = df_us_ml_data['comp_str_eng_1'].astype(int)\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_ENG2'] == True, 'TEMP_STR_ENG2'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_ENG2'].isnull(), 'TEMP_STR_ENG2'] = 0\n",
        "df_us_ml_data['comp_str_eng_2'] = df_us_data_raw['TEMP_STR_ENG2'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_eng_2'] = df_us_ml_data['comp_str_eng_2'].astype(int)\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_ENG3'] == True, 'TEMP_STR_ENG3'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_ENG3'].isnull(), 'TEMP_STR_ENG3'] = 0\n",
        "df_us_ml_data['comp_str_eng_3'] = df_us_data_raw['TEMP_STR_ENG3'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_eng_3'] = df_us_ml_data['comp_str_eng_3'].astype(int)\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_ENG4'] == True, 'TEMP_STR_ENG4'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_ENG4'].isnull(), 'TEMP_STR_ENG4'] = 0\n",
        "df_us_ml_data['comp_str_eng_4'] = df_us_data_raw['TEMP_STR_ENG4'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_eng_4'] = df_us_ml_data['comp_str_eng_4'].astype(int)\n",
        "\n",
        "# Propellers\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_PROP'] == True, 'TEMP_STR_PROP'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_PROP'].isnull(), 'TEMP_STR_PROP'] = 0\n",
        "df_us_ml_data['comp_str_prop'] = df_us_data_raw['TEMP_STR_PROP'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_prop'] = df_us_ml_data['comp_str_prop'].astype(int)\n",
        "\n",
        "# Wings & Rotors\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_WING_ROT'] == True, 'TEMP_STR_WING'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_WING'].isnull(), 'TEMP_STR_WING'] = 0\n",
        "df_us_ml_data['comp_str_wing_rotor'] = df_us_data_raw['TEMP_STR_WING'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_wing_rotor'] = df_us_ml_data['comp_str_wing_rotor'].astype(int)\n",
        "\n",
        "# Fuselage\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_FUSE'] == True, 'TEMP_STR_FUSE'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_FUSE'].isnull(), 'TEMP_STR_FUSE'] = 0\n",
        "df_us_ml_data['comp_str_fuselage'] = df_us_data_raw['TEMP_STR_FUSE'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_fuselage'] = df_us_ml_data['comp_str_fuselage'].astype(int)\n",
        "\n",
        "# Tail\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_TAIL'] == True, 'TEMP_STR_TAIL'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_TAIL'].isnull(), 'TEMP_STR_TAIL'] = 0\n",
        "df_us_ml_data['comp_str_tail'] = df_us_data_raw['TEMP_STR_TAIL'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_tail'] = df_us_ml_data['comp_str_tail'].astype(int)\n",
        "\n",
        "# Landing Gear\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_LG'] == True, 'TEMP_STR_LG'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_LG'].isnull(), 'TEMP_STR_LG'] = 0\n",
        "df_us_ml_data['comp_str_gear'] = df_us_data_raw['TEMP_STR_LG'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_gear'] = df_us_ml_data['comp_str_gear'].astype(int)\n",
        "\n",
        "# Lights\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_LGHTS'] == True, 'TEMP_STR_LGHTS'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_LGHTS'].isnull(), 'TEMP_STR_LGHTS'] = 0\n",
        "df_us_ml_data['comp_str_lights'] = df_us_data_raw['TEMP_STR_LGHTS'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_lights'] = df_us_ml_data['comp_str_lights'].astype(int)\n",
        "\n",
        "# Other\n",
        "df_us_data_raw.loc[df_us_data_raw['STR_OTHER'] == True, 'TEMP_STR_OTHER'] = 1\n",
        "df_us_data_raw.loc[df_us_data_raw['TEMP_STR_OTHER'].isnull(), 'TEMP_STR_OTHER'] = 0\n",
        "df_us_ml_data['comp_str_other'] = df_us_data_raw['TEMP_STR_OTHER'] # now copy this column into the final ML dataframe\n",
        "df_us_ml_data['comp_str_other'] = df_us_ml_data['comp_str_other'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP5GWYrhsfz_"
      },
      "outputs": [],
      "source": [
        "### Other items for pre-processing in the US Data only\n",
        "\n",
        "## The following fields are not found in the AUS data but to provide an indication of the ML algorithm's validity when limited to AUS data fields we need to test both\n",
        "\n",
        "## US Data\n",
        "\n",
        "# Pilot warned\n",
        "## This dictionary is based on field 0145 in the IBIS Manual (Doc 9332, 3rd Ed)\n",
        "warned_dict = {'Yes': 'Y',\n",
        "               'No': 'N',\n",
        "               'Unknown': 'Z'} # although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "\n",
        "df_us_data_raw.loc[df_us_data_raw['WARNED'].isnull(), 'WARNED'] = 'Unknown' # need to reassign null values to capture in standardised format\n",
        "df_us_ml_data['warned'] = df_us_data_raw['WARNED'].apply(lambda x: warned_dict.get(x)) # now copy this column into the final ML dataframe\n",
        "\n",
        "# Effect on flight\n",
        "## This feature is quoted as a categorical value in Altringer et al. (2021) but should be one hot coded bcasue some fields contain mutiple categories\n",
        "\n",
        "df_us_ml_data['effect_nil'] = 0 # let's set them all to 0 first\n",
        "df_temp_effect_nil = df_us_data_raw[df_us_data_raw['EFFECT'] == ' None'] # then get the indexes for all the records that say 'None' only\n",
        "df_us_ml_data.loc[df_temp_effect_nil.index, 'effect_nil'] = 1 # and then we switch the records with this value to 1\n",
        "\n",
        "df_us_ml_data['effect_pre_landing'] = 0 # let's set them all to 0 first\n",
        "df_temp_effect_pre_landing = df_us_data_raw[df_us_data_raw['EFFECT'].str.contains('Precautionary Landing') == True] # then get the indexes for all the records that including this text in them\n",
        "df_us_ml_data.loc[df_temp_effect_pre_landing.index, 'effect_pre_landing'] = 1 # and then we switch the records with this value to 1\n",
        "\n",
        "df_us_ml_data['effect_abort_to'] = 0 # let's set them all to 0 first\n",
        "df_temp_effect_abort_to = df_us_data_raw[df_us_data_raw['EFFECT'].str.contains('Aborted Take-off') == True] # then get the indexes for all the records that including this text in them\n",
        "df_us_ml_data.loc[df_temp_effect_abort_to.index, 'effect_abort_to'] = 1 # and then we switch the records with this value to 1\n",
        "\n",
        "df_us_ml_data['effect_eng_sd'] = 0 # let's set them all to 0 first\n",
        "df_temp_effect_eng_sd = df_us_data_raw[df_us_data_raw['EFFECT'].str.contains('Engine Shutdown') == True] # then get the indexes for all the records that including this text in them\n",
        "df_us_ml_data.loc[df_temp_effect_eng_sd.index, 'effect_eng_sd'] = 1 # and then we switch the records with this value to 1\n",
        "\n",
        "df_us_ml_data['effect_other'] = 0 # let's set them all to 0 first\n",
        "df_temp_effect_other = df_us_data_raw[df_us_data_raw['EFFECT'].str.contains('Other') == True] # then get the indexes for all the records that including this text in them\n",
        "df_us_ml_data.loc[df_temp_effect_other.index, 'effect_other'] = 1 # and then we switch the records with this value to 1\n",
        "\n",
        "# Cloud Cover\n",
        "## This dictionary is based on field 0137 in the IBIS Manual (Doc 9332, 3rd Ed)\n",
        "sky_dict = {'No Cloud': 'A',\n",
        "               'Some Cloud': 'B',\n",
        "               'Overcast': 'C',\n",
        "               'Unknown': 'Z'} # although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "\n",
        "df_us_data_raw.loc[df_us_data_raw['SKY'].isnull(), 'SKY'] = 'Unknown' # need to reassign null values to capture in standardised format\n",
        "df_us_ml_data['sky'] = df_us_data_raw['SKY'].apply(lambda x: sky_dict.get(x)) # now copy this column into the final ML dataframe\n",
        "\n",
        "# Time of Day\n",
        "## This dictionary is based on field 0110 in the IBIS Manual (Doc 9332, 3rd Ed)\n",
        "light_dict = {'Dawn': 'A',\n",
        "              'Day': 'B',\n",
        "              'Dusk': 'C',\n",
        "              'Night': 'D',\n",
        "              'Unknown': 'Z'} # although assigned blank in Doc 9332, this label is assigned 'Z' in this data\n",
        "\n",
        "df_us_data_raw.loc[df_us_data_raw['TIME_OF_DAY'].isnull(), 'TIME_OF_DAY'] = 'Unknown' # need to reassign null values to capture in standardised format\n",
        "df_us_ml_data['light_conditions'] = df_us_data_raw['TIME_OF_DAY'].apply(lambda x: light_dict.get(x)) # now copy this column into the final ML dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjC2hcj0yuxe"
      },
      "outputs": [],
      "source": [
        "### And now the target variables\n",
        "\n",
        "## US Data Only\n",
        "# Need to remove the zero values to avoid -inf in log transform\n",
        "df_us_data_raw.loc[df_us_data_raw['COST_REPAIRS_INFL_ADJ'] == 0, 'COST_REPAIRS_INFL_ADJ'] = np.nan\n",
        "df_us_data_raw.loc[df_us_data_raw['COST_OTHER_INFL_ADJ'] == 0, 'COST_OTHER_INFL_ADJ'] = 0.01\n",
        "\n",
        "# normal\n",
        "df_us_ml_data['cost_repairs_adj'] = df_us_data_raw['COST_REPAIRS_INFL_ADJ']\n",
        "df_us_ml_data['cost_other_adj'] = df_us_data_raw['COST_OTHER_INFL_ADJ']\n",
        "\n",
        "# log transformed\n",
        "df_us_ml_data['log_cost_repairs_adj'] = np.log(df_us_data_raw['COST_REPAIRS_INFL_ADJ'])\n",
        "df_us_ml_data['log_cost_other_adj'] = np.log(df_us_data_raw['COST_OTHER_INFL_ADJ'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Finally, let's now create our datasets for use in the following model development sections\n",
        "\n",
        "#### This cell is for full-one hot encoding ####\n",
        "\n",
        "## US Data\n",
        "\n",
        "# The first step is to filter for reports according to damage\n",
        "# Only reports indicating damage are included in the damage cost model\n",
        "\n",
        "l_damage_inc = ['M', 'M?', 'S', 'D']\n",
        "\n",
        "df_us_ml_damaged_inc = df_us_ml_data[df_us_ml_data['damage_level'].isin(l_damage_inc)]\n",
        "\n",
        "# The following creates the required sub-sets for replication of the Altringer et al (2021) RF modelling technique\n",
        "\n",
        "# From these sub-sets we need to further filter for only those records with reported costs\n",
        "\n",
        "df_repair_inc = df_us_ml_damaged_inc[df_us_ml_damaged_inc['log_cost_repairs_adj'].notnull()]\n",
        "df_other_inc = df_us_ml_data[df_us_ml_data['log_cost_other_adj'].notnull()]\n",
        "\n",
        "# And now we need dummy variables (OHE) for all of our categorical data\n",
        "\n",
        "# Replication Structure - Repair Costs\n",
        "X_rep_repair_inc = pd.get_dummies(df_repair_inc, prefix_sep='_') # OHE for categorical variables\n",
        "X_rep_repair_inc = X_rep_repair_inc.drop(['year', 'comp_dam_rad_nose', 'cost_repairs_adj', 'cost_other_adj', 'log_cost_repairs_adj', 'log_cost_other_adj'], axis=1) # dropping target and other variables\n",
        "y_rep_repair_inc = df_repair_inc['log_cost_repairs_adj']\n",
        "\n",
        "# Replication Structure - Other Costs\n",
        "X_rep_other_inc = pd.get_dummies(df_other_inc, prefix_sep='_') # OHC for categorical variables\n",
        "X_rep_other_inc = X_rep_other_inc.drop(['year', 'comp_dam_rad_nose', 'cost_repairs_adj', 'cost_other_adj', 'log_cost_repairs_adj', 'log_cost_other_adj'], axis=1) # dropping target and other variables\n",
        "y_rep_other_inc = df_other_inc['log_cost_other_adj']\n",
        "\n",
        "\n",
        "# Constrained Structure set up\n",
        "\n",
        "# These features are not included in the AUS data\n",
        "l_drop_features = ['comp_dam_rad', 'comp_dam_nose', 'comp_dam_eng_1', 'comp_dam_eng_2', 'comp_dam_eng_3', 'comp_dam_eng_4',\n",
        "                  'comp_str_rad', 'comp_str_nose','comp_str_wind', 'comp_str_eng_1', 'comp_str_eng_2', 'comp_str_eng_3', 'comp_str_eng_4',\n",
        "                  'comp_str_prop', 'comp_str_wing_rotor', 'comp_str_fuselage', 'comp_str_tail', 'comp_str_gear', 'comp_str_lights',\n",
        "                  'comp_str_other','warned', 'effect_nil', 'effect_pre_landing', 'effect_abort_to', 'effect_eng_sd', 'effect_other', 'sky', 'light_conditions']\n",
        "\n",
        "# These variables (feature categories) are not included in the AUS data\n",
        "l_drop_variables = ['type_eng_C', 'birds_seen_D', 'birds_struck_D']\n",
        "\n",
        "# Constrained Structure - Repair Costs\n",
        "X_cons_repair_inc = df_repair_inc.drop(l_drop_features, axis=1)\n",
        "X_cons_repair_inc = pd.get_dummies(X_cons_repair_inc, prefix_sep='_')\n",
        "X_cons_repair_inc = X_cons_repair_inc.drop(['year', 'cost_repairs_adj', 'cost_other_adj', 'log_cost_repairs_adj', 'log_cost_other_adj'], axis=1)\n",
        "X_cons_repair_inc.columns = X_cons_repair_inc.columns.str.replace('  ', '')\n",
        "X_cons_repair_inc = X_cons_repair_inc.drop(l_drop_variables, axis=1)\n",
        "y_cons_repair_inc = df_repair_inc['log_cost_repairs_adj']\n",
        "\n",
        "# Constrained Structure - Other Costs\n",
        "X_cons_other_inc = df_other_inc.drop(l_drop_features, axis=1)\n",
        "X_cons_other_inc = pd.get_dummies(X_cons_other_inc, prefix_sep='_')\n",
        "X_cons_other_inc = X_cons_other_inc.drop(['year', 'cost_repairs_adj', 'cost_other_adj', 'log_cost_repairs_adj', 'log_cost_other_adj'], axis=1)\n",
        "X_cons_other_inc.columns = X_cons_other_inc.columns.str.replace('  ', '')\n",
        "X_cons_other_inc = X_cons_other_inc.drop(l_drop_variables, axis=1)\n",
        "y_cons_other_inc = df_other_inc['log_cost_other_adj']\n",
        "\n",
        "\n",
        "## And now we need to segregate into training & test samples (80/20)\n",
        "\n",
        "X_rep_repair_inc_train, X_rep_repair_inc_test, y_rep_repair_inc_train, y_rep_repair_inc_test = train_test_split(X_rep_repair_inc, y_rep_repair_inc, test_size=0.2, random_state=23)\n",
        "X_rep_other_inc_train, X_rep_other_inc_test, y_rep_other_inc_train, y_rep_other_inc_test = train_test_split(X_rep_other_inc, y_rep_other_inc, test_size=0.2, random_state=23)\n",
        "\n",
        "X_cons_repair_inc_train, X_cons_repair_inc_test, y_cons_repair_inc_train, y_cons_repair_inc_test = train_test_split(X_cons_repair_inc, y_cons_repair_inc, test_size=0.2, random_state=23)\n",
        "X_cons_other_inc_train, X_cons_other_inc_test, y_cons_other_inc_train, y_cons_other_inc_test = train_test_split(X_cons_other_inc, y_cons_other_inc, test_size=0.2, random_state=23)\n"
      ],
      "metadata": {
        "id": "TzBA9MfPM3OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Preparation** #"
      ],
      "metadata": {
        "id": "DtgPJuevDJdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Cells ##\n",
        "\n",
        "This section includes cells that are used repeatedly in each section below"
      ],
      "metadata": {
        "id": "XjtgJmXvk_Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### This is a randomised grid for use in model development\n",
        "\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1800, num = 9)] # Number of trees in random forest\n",
        "max_features = ['sqrt'] # Number of features to consider at every split\n",
        "max_features.append(None)\n",
        "criterion = ['squared_error']\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree\n",
        "max_depth.append(None)\n",
        "min_samples_split = [int(x) for x in np.linspace(start = 2, stop = 10, num = 9)] # Minimum number of samples required to split a node\n",
        "min_samples_leaf = [1, 2, 4] # Minimum number of samples required at each leaf node\n",
        "bootstrap = [True, False] # Method of selecting samples for training each tree\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'criterion': criterion,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "pprint.pprint(random_grid) # Just for an overview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUGcBKlEDH1k",
        "outputId": "737a95e0-8633-4087-9efc-af661eb0b1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bootstrap': [True, False],\n",
            " 'criterion': ['squared_error'],\n",
            " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
            " 'max_features': ['sqrt', None],\n",
            " 'min_samples_leaf': [1, 2, 4],\n",
            " 'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
            " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor()"
      ],
      "metadata": {
        "id": "j6i_AaVMDzV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Search CV ##\n",
        "\n",
        "These cells conduct a randomised search for the best model hyperparameters using cross-validation."
      ],
      "metadata": {
        "id": "jTBmuVLpRdwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Replication - Repair Costs\n",
        "\n",
        "rf_random_rep_repair_inc = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=1, random_state=23, n_jobs = -1)\n",
        "rf_random_rep_repair_inc.fit(X_rep_repair_inc_train, y_rep_repair_inc_train)\n",
        "\n",
        "rf_random_rep_repair_inc.best_params_ # This returns the best parameters for use in the refined grid search"
      ],
      "metadata": {
        "id": "udDOKHKhRgZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Replication - Other Costs\n",
        "\n",
        "rf_random_rep_other_inc = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=1, random_state=23, n_jobs = -1)\n",
        "rf_random_rep_other_inc.fit(X_rep_other_inc_train, y_rep_other_inc_train)\n",
        "\n",
        "rf_random_rep_other_inc.best_params_ # This returns the best parameters for use in the refined grid search"
      ],
      "metadata": {
        "id": "NCu--g98rou5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Constrained - Repair Costs\n",
        "\n",
        "rf_random_cons_repair_inc = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=1, random_state=23, n_jobs = -1)\n",
        "rf_random_cons_repair_inc.fit(X_cons_repair_inc_train, y_cons_repair_inc_train)\n",
        "\n",
        "rf_random_cons_repair_inc.best_params_ # This returns the best parameters for use in the refined grid search"
      ],
      "metadata": {
        "id": "GXcZJsr2mBlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Constrained - Other Costs\n",
        "\n",
        "rf_random_cons_other_inc = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=1, random_state=23, n_jobs = -1)\n",
        "rf_random_cons_other_inc.fit(X_cons_other_inc_train, y_cons_other_inc_train)\n",
        "\n",
        "rf_random_cons_other_inc.best_params_ # This returns the best parameters for use in the refined grid search"
      ],
      "metadata": {
        "id": "OBHEnzn-mBZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search CV ##\n",
        "\n",
        "These cells undertake a more methodical grid search for the best hyperparameters using those found in the randomised search."
      ],
      "metadata": {
        "id": "cSbxndUhvdej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## More refined grid search with cross validation\n",
        "\n",
        "## based on rf_random's best parameters\n",
        "refined_grid_rep_repair_inc = {'n_estimators': [1700, 1800, 1900, 2000],\n",
        "                               'max_features': ['sqrt'],\n",
        "                               'criterion': ['squared_error'],\n",
        "                               'max_depth': [40],\n",
        "                               'min_samples_split': [6],\n",
        "                               'min_samples_leaf': [2],\n",
        "                               'bootstrap': [False]}\n",
        "\n",
        "rf_grid_rep_repair_inc = GridSearchCV(estimator = rf, param_grid = refined_grid_rep_repair_inc, cv = 10, n_jobs = -1, verbose = 2)\n",
        "\n",
        "rf_grid_rep_repair_inc.fit(X_rep_repair_inc_train, y_rep_repair_inc_train)\n",
        "\n",
        "rf_grid_rep_repair_inc.best_params_ # This returns the best parameters for use in the final model"
      ],
      "metadata": {
        "id": "ZOIb9jKyvgxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## More refined grid search with cross validation\n",
        "\n",
        "## based on rf_random's best parameters\n",
        "refined_grid_rep_other_inc = {'n_estimators': [1500, 1600, 1700],\n",
        "                               'max_features': ['sqrt'],\n",
        "                               'criterion': ['squared_error'],\n",
        "                               'max_depth': [20],\n",
        "                               'min_samples_split': [8],\n",
        "                               'min_samples_leaf': [2],\n",
        "                               'bootstrap': [False]}\n",
        "\n",
        "rf_grid_rep_other_inc = GridSearchCV(estimator = rf, param_grid = refined_grid_rep_other_inc, cv = 10, n_jobs = -1, verbose = 2)\n",
        "\n",
        "rf_grid_rep_other_inc.fit(X_rep_other_inc_train, y_rep_other_inc_train)\n",
        "\n",
        "rf_grid_rep_other_inc.best_params_ # This returns the best parameters for use in the final model"
      ],
      "metadata": {
        "id": "wPejvM2XwmWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## More refined grid search with cross validation\n",
        "\n",
        "## based on rf_random's best parameters\n",
        "refined_grid_cons_repair_inc = {'n_estimators': [500, 600, 700],\n",
        "                               'max_features': ['sqrt'],\n",
        "                               'criterion': ['squared_error'],\n",
        "                               'max_depth': [40],\n",
        "                               'min_samples_split': [5],\n",
        "                               'min_samples_leaf': [2],\n",
        "                               'bootstrap': [False]}\n",
        "\n",
        "rf_grid_cons_repair_inc = GridSearchCV(estimator = rf, param_grid = refined_grid_cons_repair_inc, cv = 10, n_jobs = -1, verbose = 2)\n",
        "\n",
        "rf_grid_cons_repair_inc.fit(X_cons_repair_inc_train, y_cons_repair_inc_train)\n",
        "\n",
        "rf_grid_cons_repair_inc.best_params_ # This returns the best parameters for use in the final model"
      ],
      "metadata": {
        "id": "hTnolsC6qxsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## More refined grid search with cross validation\n",
        "\n",
        "## based on rf_random's best parameters\n",
        "refined_grid_cons_other_inc = {'n_estimators': [900],\n",
        "                               'max_features': ['sqrt'],\n",
        "                               'criterion': ['squared_error'],\n",
        "                               'max_depth': [130, 150, 170],\n",
        "                               'min_samples_split': [5],\n",
        "                               'min_samples_leaf': [4],\n",
        "                               'bootstrap': [False]}\n",
        "\n",
        "rf_grid_cons_other_inc = GridSearchCV(estimator = rf, param_grid = refined_grid_cons_other_inc, cv = 10, n_jobs = -1, verbose = 2)\n",
        "\n",
        "rf_grid_cons_other_inc.fit(X_cons_other_inc_train, y_cons_other_inc_train)\n",
        "\n",
        "rf_grid_cons_other_inc.best_params_ # This returns the best parameters for use in the final model"
      ],
      "metadata": {
        "id": "VeOatKbCqxs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu6gFK19zwQs"
      },
      "source": [
        "# **Model Development (Replication)** #\n",
        "\n",
        "This section contains the code for the development of the RF model for repair costs. Essentially, it is a replication of the work by Altringer et al (2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwmlG1Tej0W"
      },
      "source": [
        "## Repair Costs ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh65rmXRFLq5"
      },
      "outputs": [],
      "source": [
        "## Final model trained on final parameters\n",
        "\n",
        "rf_final_rep_repair_inc = RandomForestRegressor(n_estimators=1900,\n",
        "                                 criterion='squared_error',\n",
        "                                 max_depth=40,\n",
        "                                 max_features='sqrt',\n",
        "                                 min_samples_leaf=2,\n",
        "                                 min_samples_split=6,\n",
        "                                 bootstrap=False,\n",
        "                                 random_state=23)\n",
        "\n",
        "rf_final_rep_repair_inc.fit(X_rep_repair_inc_train, y_rep_repair_inc_train)\n",
        "\n",
        "y_pred_rep_repair_inc = rf_final_rep_repair_inc.predict(X_rep_repair_inc_test) # This will 'predict' the repair costs for our test sample and allow us to compare\n",
        "\n",
        "## Tabulate the results of the model prediction against the\n",
        "\n",
        "df_results_rep_repair_inc = pd.DataFrame() # Blank dataframe for our test results\n",
        "\n",
        "df_results_rep_repair_inc['log_costs'] = y_rep_repair_inc_test\n",
        "df_results_rep_repair_inc['log_predictions'] = y_pred_rep_repair_inc\n",
        "df_results_rep_repair_inc['log_error'] = df_results_rep_repair_inc['log_predictions'] - df_results_rep_repair_inc['log_costs']\n",
        "df_results_rep_repair_inc['sq_log_error'] = df_results_rep_repair_inc['log_error'] ** 2\n",
        "\n",
        "df_results_rep_repair_inc['costs'] = np.exp(1) ** y_rep_repair_inc_test\n",
        "df_results_rep_repair_inc['predictions'] = np.exp(1) ** y_pred_rep_repair_inc\n",
        "df_results_rep_repair_inc['error'] = df_results_rep_repair_inc['predictions'] - df_results_rep_repair_inc['costs']\n",
        "df_results_rep_repair_inc['sq_error'] = df_results_rep_repair_inc['error'] ** 2\n",
        "\n",
        "### Tabulation of stratified test result calculations ###\n",
        "\n",
        "df_strat_results_rep_repair_inc = pd.DataFrame(columns=['mse', 'mae', 'r_2'])\n",
        "\n",
        "n_sample = int(len(df_results_rep_repair_inc) * 0.65)\n",
        "\n",
        "for i in range(0, 100):\n",
        "  random_records = random.sample(list(df_results_rep_repair_inc.index), n_sample)\n",
        "  df_sample = df_results_rep_repair_inc[df_results_rep_repair_inc.index.isin(random_records)]\n",
        "\n",
        "  temp_mse = df_sample['sq_log_error'].mean()\n",
        "  temp_mae = abs(df_sample['log_error']).mean()\n",
        "  temp_r_2 = r2_score(df_sample['log_costs'], df_sample['log_predictions'])\n",
        "\n",
        "  new_row = {'mse': temp_mse, 'mae': temp_mae, 'r_2': temp_r_2}\n",
        "\n",
        "  df_strat_results_rep_repair_inc = df_strat_results_rep_repair_inc.append(new_row, ignore_index=True)\n",
        "\n",
        "total_mse_mean = df_strat_results_rep_repair_inc['mse'].mean()\n",
        "total_mse_sd = df_strat_results_rep_repair_inc['mse'].std()\n",
        "total_mae_mean = df_strat_results_rep_repair_inc['mae'].mean()\n",
        "total_mae_sd = df_strat_results_rep_repair_inc['mae'].std()\n",
        "total_r_2_mean = df_strat_results_rep_repair_inc['r_2'].mean()\n",
        "total_r_2_sd = df_strat_results_rep_repair_inc['r_2'].std()\n",
        "\n",
        "print('Final Results - Replication - Repair Costs')\n",
        "print('MSE Mean (SD): ' + str(np.round(total_mse_mean, 3)) + ' (' + str(np.round(total_mse_sd, 3)) + ')')\n",
        "print('MAE Mean (SD): ' + str(np.round(total_mae_mean, 3)) + ' (' + str(np.round(total_mae_sd, 3)) + ')')\n",
        "print('R2 Mean (SD): ' + str(np.round(total_r_2_mean, 3)) + ' (' + str(np.round(total_r_2_sd, 3)) + ')')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-CmBU4FW446"
      },
      "source": [
        "## Other Costs ##"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Final model trained on final parameters\n",
        "\n",
        "rf_final_rep_other_inc = RandomForestRegressor(n_estimators=1700,\n",
        "                                 criterion='squared_error',\n",
        "                                 max_depth=150,\n",
        "                                 max_features='sqrt',\n",
        "                                 min_samples_leaf=2,\n",
        "                                 min_samples_split=8,\n",
        "                                 bootstrap=False,\n",
        "                                 random_state=23)\n",
        "\n",
        "rf_final_rep_other_inc.fit(X_rep_other_inc_train, y_rep_other_inc_train)\n",
        "\n",
        "y_pred_rep_other_inc = rf_final_rep_other_inc.predict(X_rep_other_inc_test) # This will 'predict' the repair costs for our test sample and allow us to compare\n",
        "\n",
        "\n",
        "## Tabulate the results of the model prediction against the\n",
        "\n",
        "df_results_rep_other_inc = pd.DataFrame() # Blank dataframe for our test results\n",
        "\n",
        "df_results_rep_other_inc['log_costs'] = y_rep_other_inc_test\n",
        "df_results_rep_other_inc['log_predictions'] = y_pred_rep_other_inc\n",
        "df_results_rep_other_inc['log_error'] = df_results_rep_other_inc['log_predictions'] - df_results_rep_other_inc['log_costs']\n",
        "df_results_rep_other_inc['sq_log_error'] = df_results_rep_other_inc['log_error'] ** 2\n",
        "\n",
        "df_results_rep_other_inc['costs'] = np.exp(1) ** y_rep_other_inc_test\n",
        "df_results_rep_other_inc['predictions'] = np.exp(1) ** y_pred_rep_other_inc\n",
        "df_results_rep_other_inc['error'] = df_results_rep_other_inc['predictions'] - df_results_rep_other_inc['costs']\n",
        "df_results_rep_other_inc['sq_error'] = df_results_rep_other_inc['error'] ** 2\n",
        "\n",
        "\n",
        "### Tabulation of stratified test result calculations ###\n",
        "\n",
        "df_strat_results_rep_other_inc = pd.DataFrame(columns=['mse', 'mae', 'r_2'])\n",
        "\n",
        "n_sample = int(len(df_results_rep_other_inc) * 0.65)\n",
        "\n",
        "for i in range(0, 100):\n",
        "  random_records = random.sample(list(df_results_rep_other_inc.index), n_sample)\n",
        "  df_sample = df_results_rep_other_inc[df_results_rep_other_inc.index.isin(random_records)]\n",
        "\n",
        "  temp_mse = df_sample['sq_log_error'].mean()\n",
        "  temp_mae = abs(df_sample['log_error']).mean()\n",
        "  temp_r_2 = r2_score(df_sample['log_costs'], df_sample['log_predictions'])\n",
        "\n",
        "  new_row = {'mse': temp_mse, 'mae': temp_mae, 'r_2': temp_r_2}\n",
        "\n",
        "  df_strat_results_rep_other_inc = df_strat_results_rep_other_inc.append(new_row, ignore_index=True)\n",
        "\n",
        "total_mse_mean = df_strat_results_rep_other_inc['mse'].mean()\n",
        "total_mse_sd = df_strat_results_rep_other_inc['mse'].std()\n",
        "total_mae_mean = df_strat_results_rep_other_inc['mae'].mean()\n",
        "total_mae_sd = df_strat_results_rep_other_inc['mae'].std()\n",
        "total_r_2_mean = df_strat_results_rep_other_inc['r_2'].mean()\n",
        "total_r_2_sd = df_strat_results_rep_other_inc['r_2'].std()\n",
        "\n",
        "print('Final Results - Replication - Other Costs')\n",
        "print('MSE Mean (SD): ' + str(np.round(total_mse_mean, 3)) + ' (' + str(np.round(total_mse_sd, 3)) + ')')\n",
        "print('MAE Mean (SD): ' + str(np.round(total_mae_mean, 3)) + ' (' + str(np.round(total_mae_sd, 3)) + ')')\n",
        "print('R2 Mean (SD): ' + str(np.round(total_r_2_mean, 3)) + ' (' + str(np.round(total_r_2_sd, 3)) + ')')"
      ],
      "metadata": {
        "id": "AApXcAue49Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Constrained US Model (based on AUS data structure)** #"
      ],
      "metadata": {
        "id": "1J9DH05ivdCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repair Costs ##"
      ],
      "metadata": {
        "id": "oD8MyNGVtdfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Final model trained on final parameters\n",
        "\n",
        "rf_final_cons_repair_inc = RandomForestRegressor(n_estimators=600,\n",
        "                                 criterion='squared_error',\n",
        "                                 max_depth=40,\n",
        "                                 max_features='sqrt',\n",
        "                                 min_samples_leaf=2,\n",
        "                                 min_samples_split=5,\n",
        "                                 bootstrap=False,\n",
        "                                 random_state=23)\n",
        "\n",
        "rf_final_cons_repair_inc.fit(X_cons_repair_inc_train, y_cons_repair_inc_train)\n",
        "\n",
        "y_pred_cons_repair_inc = rf_final_cons_repair_inc.predict(X_cons_repair_inc_test) # This will 'predict' the costs for our test sample and allow us to compare\n",
        "\n",
        "## Tabulate the results of the model prediction against the\n",
        "\n",
        "df_results_cons_repair_inc = pd.DataFrame() # Blank dataframe for our test results\n",
        "\n",
        "df_results_cons_repair_inc['log_costs'] = y_cons_repair_inc_test\n",
        "df_results_cons_repair_inc['log_predictions'] = y_pred_cons_repair_inc\n",
        "df_results_cons_repair_inc['log_error'] = df_results_cons_repair_inc['log_predictions'] - df_results_cons_repair_inc['log_costs']\n",
        "df_results_cons_repair_inc['sq_log_error'] = df_results_cons_repair_inc['log_error'] ** 2\n",
        "\n",
        "df_results_cons_repair_inc['costs'] = np.exp(1) ** y_cons_repair_inc_test\n",
        "df_results_cons_repair_inc['predictions'] = np.exp(1) ** y_pred_cons_repair_inc\n",
        "df_results_cons_repair_inc['error'] = df_results_cons_repair_inc['predictions'] - df_results_cons_repair_inc['costs']\n",
        "df_results_cons_repair_inc['sq_error'] = df_results_cons_repair_inc['error'] ** 2\n",
        "\n",
        "### Tabulation of stratified test result calculations ###\n",
        "\n",
        "df_strat_results_cons_repair_inc = pd.DataFrame(columns=['mse', 'mae', 'r_2'])\n",
        "\n",
        "n_sample = int(len(df_results_cons_repair_inc) * 0.65)\n",
        "\n",
        "for i in range(0, 100):\n",
        "  random_records = random.sample(list(df_results_cons_repair_inc.index), n_sample)\n",
        "  df_sample = df_results_cons_repair_inc[df_results_cons_repair_inc.index.isin(random_records)]\n",
        "\n",
        "  temp_mse = df_sample['sq_log_error'].mean()\n",
        "  temp_mae = abs(df_sample['log_error']).mean()\n",
        "  temp_r_2 = r2_score(df_sample['log_costs'], df_sample['log_predictions'])\n",
        "\n",
        "  new_row = {'mse': temp_mse, 'mae': temp_mae, 'r_2': temp_r_2}\n",
        "\n",
        "  df_strat_results_cons_repair_inc = df_strat_results_cons_repair_inc.append(new_row, ignore_index=True)\n",
        "\n",
        "total_mse_mean = df_strat_results_cons_repair_inc['mse'].mean()\n",
        "total_mse_sd = df_strat_results_cons_repair_inc['mse'].std()\n",
        "total_mae_mean = df_strat_results_cons_repair_inc['mae'].mean()\n",
        "total_mae_sd = df_strat_results_cons_repair_inc['mae'].std()\n",
        "total_r_2_mean = df_strat_results_cons_repair_inc['r_2'].mean()\n",
        "total_r_2_sd = df_strat_results_cons_repair_inc['r_2'].std()\n",
        "\n",
        "print('Final Results - Constrained - Repair Costs')\n",
        "print('MSE Mean (SD): ' + str(np.round(total_mse_mean, 3)) + ' (' + str(np.round(total_mse_sd, 3)) + ')')\n",
        "print('MAE Mean (SD): ' + str(np.round(total_mae_mean, 3)) + ' (' + str(np.round(total_mae_sd, 3)) + ')')\n",
        "print('R2 Mean (SD): ' + str(np.round(total_r_2_mean, 3)) + ' (' + str(np.round(total_r_2_sd, 3)) + ')')"
      ],
      "metadata": {
        "id": "QPW6qSG4HuXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkGA6WaKvxu1"
      },
      "source": [
        "## Other Costs ##"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Final model trained on final parameters\n",
        "\n",
        "rf_final_cons_other_inc = RandomForestRegressor(n_estimators=900,\n",
        "                                 criterion='squared_error',\n",
        "                                 max_depth=170,\n",
        "                                 max_features='sqrt',\n",
        "                                 min_samples_leaf=4,\n",
        "                                 min_samples_split=5,\n",
        "                                 bootstrap=False,\n",
        "                                 random_state=23)\n",
        "\n",
        "rf_final_cons_other_inc.fit(X_cons_other_inc_train, y_cons_other_inc_train)\n",
        "\n",
        "y_pred_cons_other_inc = rf_final_cons_other_inc.predict(X_cons_other_inc_test) # This will 'predict' the repair costs for our test sample and allow us to compare\n",
        "\n",
        "\n",
        "## Tabulate the results of the model prediction against the\n",
        "\n",
        "df_results_cons_other_inc = pd.DataFrame() # Blank dataframe for our test results\n",
        "\n",
        "df_results_cons_other_inc['log_costs'] = y_cons_other_inc_test\n",
        "df_results_cons_other_inc['log_predictions'] = y_pred_cons_other_inc\n",
        "df_results_cons_other_inc['log_error'] = df_results_cons_other_inc['log_predictions'] - df_results_cons_other_inc['log_costs']\n",
        "df_results_cons_other_inc['sq_log_error'] = df_results_cons_other_inc['log_error'] ** 2\n",
        "\n",
        "df_results_cons_other_inc['costs'] = np.exp(1) ** y_cons_other_inc_test\n",
        "df_results_cons_other_inc['predictions'] = np.exp(1) ** y_pred_cons_other_inc\n",
        "df_results_cons_other_inc['error'] = df_results_cons_other_inc['predictions'] - df_results_cons_other_inc['costs']\n",
        "df_results_cons_other_inc['sq_error'] = df_results_cons_other_inc['error'] ** 2\n",
        "\n",
        "\n",
        "### Tabulation of stratified test result calculations ###\n",
        "\n",
        "df_strat_results_cons_other_inc = pd.DataFrame(columns=['mse', 'mae', 'r_2'])\n",
        "\n",
        "n_sample = int(len(df_results_cons_other_inc) * 0.65)\n",
        "\n",
        "for i in range(0, 100):\n",
        "  random_records = random.sample(list(df_results_cons_other_inc.index), n_sample)\n",
        "  df_sample = df_results_cons_other_inc[df_results_cons_other_inc.index.isin(random_records)]\n",
        "\n",
        "  temp_mse = df_sample['sq_log_error'].mean()\n",
        "  temp_mae = abs(df_sample['log_error']).mean()\n",
        "  temp_r_2 = r2_score(df_sample['log_costs'], df_sample['log_predictions'])\n",
        "\n",
        "  new_row = {'mse': temp_mse, 'mae': temp_mae, 'r_2': temp_r_2}\n",
        "\n",
        "  df_strat_results_cons_other_inc = df_strat_results_cons_other_inc.append(new_row, ignore_index=True)\n",
        "\n",
        "total_mse_mean = df_strat_results_cons_other_inc['mse'].mean()\n",
        "total_mse_sd = df_strat_results_cons_other_inc['mse'].std()\n",
        "total_mae_mean = df_strat_results_cons_other_inc['mae'].mean()\n",
        "total_mae_sd = df_strat_results_cons_other_inc['mae'].std()\n",
        "total_r_2_mean = df_strat_results_cons_other_inc['r_2'].mean()\n",
        "total_r_2_sd = df_strat_results_cons_other_inc['r_2'].std()\n",
        "\n",
        "print('Final Results - Constrained - Other Costs')\n",
        "print('MSE Mean (SD): ' + str(np.round(total_mse_mean, 3)) + ' (' + str(np.round(total_mse_sd, 3)) + ')')\n",
        "print('MAE Mean (SD): ' + str(np.round(total_mae_mean, 3)) + ' (' + str(np.round(total_mae_sd, 3)) + ')')\n",
        "print('R2 Mean (SD): ' + str(np.round(total_r_2_mean, 3)) + ' (' + str(np.round(total_r_2_sd, 3)) + ')')"
      ],
      "metadata": {
        "id": "UFBXkTgAJxIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Calculation of significance scores for performance difference\n",
        "\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Repair Costs\n",
        "print('Repair MSE: p-value', round(stats.ttest_ind(a=df_strat_results_rep_repair_inc['mse'], b=df_strat_results_cons_repair_inc['mse'], equal_var=True)[1], 3))\n",
        "print('Repair MAE: p-value', round(stats.ttest_ind(a=df_strat_results_rep_repair_inc['mae'], b=df_strat_results_cons_repair_inc['mae'], equal_var=True)[1], 3))\n",
        "print('Repair R^2: p-value', round(stats.ttest_ind(a=df_strat_results_rep_repair_inc['r_2'], b=df_strat_results_cons_repair_inc['r_2'], equal_var=True)[1], 3))\n",
        "\n",
        "# Other Costs\n",
        "print('Other MSE: p-value', round(stats.ttest_ind(a=df_strat_results_rep_other_inc['mse'], b=df_strat_results_cons_other_inc['mse'], equal_var=True)[1], 3))\n",
        "print('Other MAE: p-value', round(stats.ttest_ind(a=df_strat_results_rep_other_inc['mae'], b=df_strat_results_cons_other_inc['mae'], equal_var=True)[1], 3))\n",
        "print('Other R^2: p-value', round(stats.ttest_ind(a=df_strat_results_rep_other_inc['r_2'], b=df_strat_results_cons_other_inc['r_2'], equal_var=True)[1], 3))"
      ],
      "metadata": {
        "id": "xJW41W1n_kab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualisation of Results** #"
      ],
      "metadata": {
        "id": "w5m8OISSRcWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(25, 7))\n",
        "fig.suptitle('Repair Cost Model Performance', fontsize = 20)\n",
        "\n",
        "x000 = df_strat_results_rep_repair_inc['mse']\n",
        "x001 = df_strat_results_cons_repair_inc['mse']\n",
        "\n",
        "x100 = df_strat_results_rep_repair_inc['mae']\n",
        "x101 = df_strat_results_cons_repair_inc['mae']\n",
        "\n",
        "x200 = df_strat_results_rep_repair_inc['r_2']\n",
        "x201 = df_strat_results_cons_repair_inc['r_2']\n",
        "\n",
        "sns.kdeplot(ax=axes[0], x=x000, shade=True, color='darkorange')\n",
        "sns.kdeplot(ax=axes[0], x=x001, shade=True, color='blue')\n",
        "axes[0].set_xlabel('Mean Square Error', fontsize = 15)\n",
        "axes[0].set_ylabel('', fontsize = 20)\n",
        "axes[0].axvline(2.637, color=\"black\", ls=\"--\")\n",
        "axes[0].set(yticklabels=[])\n",
        "\n",
        "sns.kdeplot(ax=axes[1], x=x100, shade=True, color='darkorange')\n",
        "sns.kdeplot(ax=axes[1], x=x101, shade=True, color='blue')\n",
        "axes[1].set_xlabel('Mean Absolute Error', fontsize = 15)\n",
        "axes[1].set_ylabel('', fontsize = 20)\n",
        "axes[1].axvline(1.244, color=\"black\", ls=\"--\")\n",
        "axes[1].set(yticklabels=[])\n",
        "\n",
        "\n",
        "sns.kdeplot(ax=axes[2], x=x200, shade=True, color='darkorange')\n",
        "sns.kdeplot(ax=axes[2], x=x201, shade=True, color='blue')\n",
        "axes[2].set_xlabel('R-squared', fontsize = 15)\n",
        "axes[2].set_ylabel('', fontsize = 20)\n",
        "axes[2].axvline(0.504, color=\"black\", ls=\"--\")\n",
        "axes[2].set(yticklabels=[])\n",
        "\n",
        "\n",
        "plt.legend(labels=['Altringer et al Results', 'Full Feature Set', 'Constrained Feature Set'])\n"
      ],
      "metadata": {
        "id": "h30_Zt1bU9nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(25, 7))\n",
        "fig.suptitle('Other Cost Model Performance', fontsize = 20)\n",
        "\n",
        "x002 = df_strat_results_rep_other_inc['mse']\n",
        "x003 = df_strat_results_cons_other_inc['mse']\n",
        "\n",
        "x102 = df_strat_results_rep_other_inc['mae']\n",
        "x103 = df_strat_results_cons_other_inc['mae']\n",
        "\n",
        "x202 = df_strat_results_rep_other_inc['r_2']\n",
        "x203 = df_strat_results_cons_other_inc['r_2']\n",
        "\n",
        "sns.kdeplot(ax=axes[0], x=x002, shade=True, color='darkorange')\n",
        "sns.kdeplot(ax=axes[0], x=x003, shade=True, color='blue')\n",
        "axes[0].set_xlabel('Mean Square Error', fontsize = 15)\n",
        "axes[0].set_ylabel('', fontsize = 20)\n",
        "axes[0].axvline(1.822, color=\"black\", ls=\"--\")\n",
        "axes[0].set(yticklabels=[])\n",
        "\n",
        "sns.kdeplot(ax=axes[1], x=x102, shade=True, color='darkorange')\n",
        "sns.kdeplot(ax=axes[1], x=x103, shade=True, color='blue')\n",
        "axes[1].set_xlabel('Mean Absolute Error', fontsize = 15)\n",
        "axes[1].set_ylabel('', fontsize = 20)\n",
        "axes[1].axvline(0.838, color=\"black\", ls=\"--\")\n",
        "axes[1].set(yticklabels=[])\n",
        "\n",
        "sns.kdeplot(ax=axes[2], x=x202, shade=True, color='darkorange')\n",
        "sns.kdeplot(ax=axes[2], x=x203, shade=True, color='blue')\n",
        "axes[2].set_xlabel('R-squared', fontsize = 15)\n",
        "axes[2].set_ylabel('', fontsize = 20)\n",
        "axes[2].axvline(0.945, color=\"black\", ls=\"--\")\n",
        "axes[2].set(yticklabels=[])\n",
        "\n",
        "\n",
        "plt.legend(labels=['Altringer et al Results', 'Full Feature Set', 'Constrained Feature Set'])\n"
      ],
      "metadata": {
        "id": "Yacs99qJi4YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Application to AUS Data** #"
      ],
      "metadata": {
        "id": "JoDMwTBve_yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## The first step is to filter for reports indicating damage\n",
        "\n",
        "l_damage_indicators = ['M', 'M?', 'S', 'D']\n",
        "df_aus_ml_damaged_only = df_aus_ml_data[df_aus_ml_data['damage_level'].isin(l_damage_indicators)]\n",
        "\n",
        "## And now we need dummy variables for all of our categorical data\n",
        "\n",
        "X_aus_features_dmg_only = pd.get_dummies(df_aus_ml_damaged_only, prefix_sep='_')\n",
        "X_aus_features_all = pd.get_dummies(df_aus_ml_data, prefix_sep='_')\n",
        "\n",
        "## And finally we need to drop some columns for data that doesn't appear in the US data\n",
        "\n",
        "X_aus_features_dmg_only = X_aus_features_dmg_only.drop(['year', 'acft_class_J', 'acft_class_R', 'type_eng_Y'], axis=1)\n",
        "X_aus_features_all = X_aus_features_all.drop(['year', 'acft_class_J', 'acft_class_R', 'type_eng_C', 'type_eng_Y'], axis=1)\n"
      ],
      "metadata": {
        "id": "6gqQKKjlfGAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repair Costs ##"
      ],
      "metadata": {
        "id": "6XdGjAzffI71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Final model trained on final parameters\n",
        "\n",
        "rf_final_aus_repair_inc = RandomForestRegressor(n_estimators=600,\n",
        "                                 criterion='squared_error',\n",
        "                                 max_depth=40,\n",
        "                                 max_features='sqrt',\n",
        "                                 min_samples_leaf=2,\n",
        "                                 min_samples_split=5,\n",
        "                                 bootstrap=False,\n",
        "                                 random_state=23)\n",
        "\n",
        "rf_final_aus_repair_inc.fit(X_cons_repair_inc, y_cons_repair_inc)\n",
        "\n",
        "aus_pred_repair_inc = rf_final_aus_repair_inc.predict(X_aus_features_dmg_only)"
      ],
      "metadata": {
        "id": "BaeoxHPaguw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aus_pred_log_mean = round(aus_pred_log.mean(), 2)\n",
        "aus_pred_log_std_dev = round(aus_pred_log.std(), 2)\n",
        "aus_pred_log_min = round(aus_pred_log.min(), 2)\n",
        "aus_pred_log_median = round(np.median(aus_pred_log), 2)\n",
        "aus_pred_log_max = round(aus_pred_log.max(), 2)\n",
        "aus_pred_log_n = len(aus_pred_log)\n",
        "\n",
        "print('Log mean:', aus_pred_log_mean)\n",
        "print('Log std dev:', aus_pred_log_std_dev)\n",
        "print('Log min:', aus_pred_log_min)\n",
        "print('Log median:', aus_pred_log_median)\n",
        "print('Log max:', aus_pred_log_max)\n",
        "print('Log n:', aus_pred_log_n)\n",
        "print('')\n",
        "\n",
        "aus_pred_mean = round(aus_pred_raw.mean())\n",
        "aus_pred_std_dev = round(aus_pred_raw.std())\n",
        "aus_pred_min = round(aus_pred_raw.min(), 2)\n",
        "aus_pred_median = round(np.median(aus_pred_raw))\n",
        "aus_pred_max = round(aus_pred_raw.max())\n",
        "aus_pred_n = len(aus_pred_raw)\n",
        "\n",
        "print('Raw mean:', aus_pred_mean)\n",
        "print('Raw std dev:', aus_pred_std_dev)\n",
        "print('Raw min:', aus_pred_min)\n",
        "print('Raw median:', aus_pred_median)\n",
        "print('Raw max:', aus_pred_max)\n",
        "print('Raw n:', aus_pred_n)\n",
        "print('')\n",
        "\n",
        "aus_pred_total_cost = aus_pred_raw.sum()\n",
        "aus_pred_avg_annual_cost = round(aus_pred_total_cost/10, 2)\n",
        "\n",
        "print('Total cost 2008-2017:', aus_pred_total_cost)\n",
        "print('Average annual cost (US$):', aus_pred_avg_annual_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVT0VGKdr-Oc",
        "outputId": "f5a28bf5-bc50-4ac2-83d9-dc5e4adf3ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log mean: 8.88\n",
            "Log std dev: 0.77\n",
            "Log min: 5.9\n",
            "Log median: 8.9\n",
            "Log max: 11.71\n",
            "Log n: 4640\n",
            "\n",
            "Raw mean: 9707\n",
            "Raw std dev: 8912\n",
            "Raw min: 365.27\n",
            "Raw median: 7343\n",
            "Raw max: 121960\n",
            "Raw n: 4640\n",
            "\n",
            "Total cost 2008-2017: 45040428.306524664\n",
            "Average annual cost (US$): 4504042.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Costs ##"
      ],
      "metadata": {
        "id": "lfKlX7tJDhfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Final model trained on final parameters\n",
        "\n",
        "rf_final_aus_other_inc = RandomForestRegressor(n_estimators=900,\n",
        "                                 criterion='squared_error',\n",
        "                                 max_depth=170,\n",
        "                                 max_features='sqrt',\n",
        "                                 min_samples_leaf=4,\n",
        "                                 min_samples_split=5,\n",
        "                                 bootstrap=False,\n",
        "                                 random_state=23)\n",
        "\n",
        "rf_final_aus_other_inc.fit(X_cons_other_inc, y_cons_other_inc)\n",
        "\n",
        "aus_pred_other_inc = rf_final_aus_other_inc.predict(X_aus_features_all)"
      ],
      "metadata": {
        "id": "kDRB7VtKDlFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aus_pred_other_log = aus_pred_other_inc\n",
        "aus_pred_other_raw = np.e ** aus_pred_other_inc\n",
        "\n",
        "aus_pred_other_log_mean = round(aus_pred_other_log.mean())\n",
        "aus_pred_other_log_std_dev = round(aus_pred_other_log.std())\n",
        "aus_pred_other_log_min = round(aus_pred_other_log.min(), 2)\n",
        "aus_pred_other_log_median = round(np.median(aus_pred_other_log))\n",
        "aus_pred_other_log_max = round(aus_pred_other_log.max())\n",
        "aus_pred_other_log_n = len(aus_pred_other_log)\n",
        "\n",
        "print('Log mean:', aus_pred_other_log_mean)\n",
        "print('Log std dev:', aus_pred_other_log_std_dev)\n",
        "print('Log min:', aus_pred_other_log_min)\n",
        "print('Log median:', aus_pred_other_log_median)\n",
        "print('Log max:', aus_pred_other_log_max)\n",
        "print('Log n:', aus_pred_other_log_n)\n",
        "print('')\n",
        "\n",
        "aus_pred_other_mean = round(aus_pred_other_raw.mean())\n",
        "aus_pred_other_std_dev = round(aus_pred_other_raw.std())\n",
        "aus_pred_other_min = round(aus_pred_other_raw.min(), 2)\n",
        "aus_pred_other_median = round(np.median(aus_pred_other_raw))\n",
        "aus_pred_other_max = round(aus_pred_other_raw.max())\n",
        "aus_pred_other_n = len(aus_pred_other_raw)\n",
        "\n",
        "print('Raw mean:', aus_pred_other_mean)\n",
        "print('Raw std dev:', aus_pred_other_std_dev)\n",
        "print('Raw min:', aus_pred_other_min)\n",
        "print('Raw median:', aus_pred_other_median)\n",
        "print('Raw max:', aus_pred_other_max)\n",
        "print('Raw n:', aus_pred_other_n)\n",
        "print('')\n",
        "\n",
        "aus_pred_other_total_cost = aus_pred_other_raw.sum()\n",
        "aus_pred_other_avg_annual_cost = round(aus_pred_other_total_cost/10, 2)\n",
        "\n",
        "print('Total cost 2008-2017:', aus_pred_other_total_cost)\n",
        "print('Average annual cost (US$):', aus_pred_other_avg_annual_cost)"
      ],
      "metadata": {
        "id": "GHK5UvxjD4w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annualised Trends ##\n",
        "\n",
        "Exported to CSV for further analysis"
      ],
      "metadata": {
        "id": "PK9hgBRfE9Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_aus_predictions = pd.DataFrame()\n",
        "\n",
        "df_aus_predictions['year'] = df_aus_ml_data['year']\n",
        "\n",
        "temp_df_aus_pred_repair = pd.DataFrame()\n",
        "temp_df_aus_pred_repair['repair_cost_predictions'] = aus_pred_raw\n",
        "temp_df_aus_pred_repair = temp_df_aus_pred_repair.set_index(df_aus_ml_damaged_only.index)\n",
        "\n",
        "df_aus_predictions['repair_cost_predictions'] = temp_df_aus_pred_repair['repair_cost_predictions']\n",
        "df_aus_predictions.loc[df_aus_predictions['repair_cost_predictions'].isnull(), 'repair_cost_predictions'] = 0\n",
        "\n",
        "df_aus_predictions['other_cost_predictions'] = aus_pred_other_raw\n",
        "\n",
        "df_aus_predictions['total_cost_predictions'] = df_aus_predictions['other_cost_predictions'] + df_aus_predictions['repair_cost_predictions']\n",
        "\n",
        "df_aus_predictions.loc[df_aus_predictions['repair_cost_predictions'] == 0, 'repair_cost_predictions'] = np.nan"
      ],
      "metadata": {
        "id": "JAE9Lbx7FAWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annual_predictions = pd.DataFrame(columns=['year', 'total_strikes', 'total_repair', 'mean_repair', 'median_repair', 'total_other', 'mean_other', 'median_other', 'total_total', 'mean_total', 'median_total'])\n",
        "\n",
        "l_years = df_aus_predictions['year'].unique().tolist()\n",
        "\n",
        "for year in l_years:\n",
        "  temp_data = df_aus_predictions[df_aus_predictions['year'] ==  year]\n",
        "\n",
        "  temp_strikes = len(temp_data)\n",
        "\n",
        "  temp_total_repair = temp_data.loc[temp_data['repair_cost_predictions'].notnull(), 'repair_cost_predictions'].sum()\n",
        "  temp_mean_repair = temp_data.loc[temp_data['repair_cost_predictions'].notnull(), 'repair_cost_predictions'].mean()\n",
        "  temp_median_repair = np.median(temp_data.loc[temp_data['repair_cost_predictions'].notnull(), 'repair_cost_predictions'])\n",
        "\n",
        "  temp_total_other = temp_data['other_cost_predictions'].sum()\n",
        "  temp_mean_other = temp_data['other_cost_predictions'].mean()\n",
        "  temp_median_other = np.median(temp_data['other_cost_predictions'])\n",
        "\n",
        "  temp_total_total = temp_data['total_cost_predictions'].sum()\n",
        "  temp_mean_total = temp_data['total_cost_predictions'].mean()\n",
        "  temp_median_total = np.median(temp_data['total_cost_predictions'])\n",
        "\n",
        "  new_row = {'year': year, 'total_strikes': temp_strikes, 'total_repair': temp_total_repair, 'mean_repair': temp_mean_repair,\n",
        "             'median_repair': temp_median_repair, 'total_other': temp_total_other,\n",
        "             'mean_other': temp_mean_other, 'median_other': temp_median_other, 'total_total': temp_total_total,\n",
        "             'mean_total': temp_mean_total, 'median_total': temp_median_total}\n",
        "\n",
        "  df_annual_predictions = df_annual_predictions.append(new_row, ignore_index=True)\n",
        "\n",
        "df_annual_predictions['year'] = df_annual_predictions['year'].astype(int)"
      ],
      "metadata": {
        "id": "fY8NT76MK_Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_annual_predictions.to_csv('final_aus_predictions.csv')"
      ],
      "metadata": {
        "id": "jG0lP7WfLH6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WrM1wESA66ch",
        "8n5r_8Px7GDF",
        "veP3uxGtWPP2",
        "QmZpZRuEWiAr",
        "41prbY68aFuQ",
        "NT-lA-M59--C",
        "wxY3wd5P_AXa",
        "lJf8CLsM3VNw",
        "a-CmBU4FW446",
        "5-OnSn0W3qto",
        "1Uww8KUI6JjN",
        "7X5R0h01ewwb",
        "EueGSUX1PBOq",
        "dAgHmQyGlJ6d",
        "iomzencWsv9Q"
      ],
      "mount_file_id": "1UKtWnZcoM41WTrXRFtC3XvK30OF5aRlU",
      "authorship_tag": "ABX9TyOw3sCA0zCSVroqxeZ38hbs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}